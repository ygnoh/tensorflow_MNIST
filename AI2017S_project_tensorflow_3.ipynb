{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI2017S project tutorial: Simple Neural Network learning MNIST\n",
    "+ Original source: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py\n",
    "+ Modified and supplemented by Seongho Son, Seongjun Choi\n",
    "\n",
    "#### Requirements\n",
    "+ python\n",
    "+ jupyter notebook\n",
    "+ tensorflow\n",
    "+ (If possible) GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries \n",
    "- Tensorflow\n",
    "- numpy\n",
    "- input_data(library for loading MNIST dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST dataset, download if does not exist\n",
    "\n",
    "+ image (handwritten digit): 28 * 28 = 784 pixels\n",
    "+ corresponding label (one of 0, 1, 2, ..., 9) in 'one_hot vector' format\n",
    "+ train / validation / test instances: 55000 / 5000 / 10000\n",
    "+ training images: 'mnist.train.images'\n",
    "+ training labels: 'mnist.train.labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(5000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('./MNIST_data', one_hot = True)\n",
    "print(mnist.train.images.shape)\n",
    "print(mnist.validation.images.shape)\n",
    "print(mnist.test.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Reset model), Define session\n",
    "\n",
    "+ __tf.reset_default_graph()__ resets your model definition, so that you don't have to restart the kernel.\n",
    "+ If you do not specify the fraction of GPU memory to be used by tensorflow in this code, it automatically assigns __all the remaining GPU memory__, which will prevent you from running separate code which uses GPU memory.\n",
    "+ You can control the portion of GPU memory used by this code, by setting __tf.GPUOptions__ like below.\n",
    "       assigning 0.2 to *per_process_gpu_memory_fraction* means that \n",
    "       20% of the whole GPU memory is assigned to this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tf.reset_default_graph()\n",
    "    sess.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Use only 20% of the whole GPU memory for this code\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n",
    "+ model parameters\n",
    "+ placeholders to contain input / target data\n",
    "+ training algorithm used for learning\n",
    "+ *__correct_prediction__* is *__None__*-size long, 1-dimentional vector whoes each element means if the prediction for the item was correct or not.\n",
    "+ *__accurarcy__* is a *__float32__* value of the average of all elements in *__correct_prediction__*. That's what *__reduce__* and *__mean__* means in *__tf.reduce_mean__*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.zeros([784, 50]))\n",
    "b1 = tf.Variable(tf.zeros([50]))\n",
    "h = tf.sigmoid(tf.matmul(x, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.zeros([50, 10]))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "y = tf.matmul(h, W2) + b2\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.3).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "+ use a placeholder for assigning __probability of using each weight__ for training\n",
    "        assign a value between 0 and 1.0 during training,\n",
    "        and assign 1.0 during validation or test\n",
    "+ use *__tf.nn.dropout__* to apply dropout on that layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1_DO = tf.Variable(tf.zeros([784, 50]))\n",
    "b1_DO = tf.Variable(tf.zeros([50]))\n",
    "h_DO = tf.sigmoid(tf.matmul(x, W1_DO) + b1_DO)\n",
    "h_drop = tf.nn.dropout(h_DO, keep_prob)\n",
    "\n",
    "W2_DO = tf.Variable(tf.zeros([50, 10]))\n",
    "b2_DO = tf.Variable(tf.zeros([10]))\n",
    "y_DO = tf.matmul(h_drop, W2_DO) + b2_DO\n",
    "y_drop = tf.nn.dropout(y_DO, keep_prob)\n",
    "\n",
    "cross_entropy_DO = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_DO))\n",
    "train_step_DO = tf.train.GradientDescentOptimizer(0.3).minimize(cross_entropy_DO)\n",
    "\n",
    "correct_prediction_DO = tf.equal(tf.argmax(y_DO, 1), tf.argmax(y_, 1))\n",
    "accuracy_DO = tf.reduce_mean(tf.cast(correct_prediction_DO, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "+ obtain mean and variance of current (mini-)batch by using __tf.nn.moments__\n",
    "        these mean and variance will be used for normalization\n",
    "+ Assign two additional vectors to each layer for learning: 'scale' and 'beta'\n",
    "+ 'epsilon' used for __preventing zero variances__\n",
    "+ *__tf.nn.batch_normalization__* used for applying batch normalization, whose result will then be the input to the layer's activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-3\n",
    "\n",
    "W1_BN = tf.Variable(tf.zeros([784, 50]))\n",
    "z1_BN = tf.matmul(x, W1_BN)\n",
    "bmean1, bvar1 = tf.nn.moments(z1_BN, [0])\n",
    "scale1 = tf.Variable(tf.ones([50]))\n",
    "beta1 = tf.Variable(tf.zeros([50]))\n",
    "BN1 = tf.nn.batch_normalization(z1_BN, bmean1, bvar1, beta1, scale1, epsilon)\n",
    "h_BN = tf.nn.sigmoid(BN1)\n",
    "\n",
    "W2_BN = tf.Variable(tf.zeros([50, 10]))\n",
    "z2_BN = tf.matmul(h_BN, W2_BN)\n",
    "bmean2, bvar2 = tf.nn.moments(z2_BN, [0])\n",
    "scale2 = tf.Variable(tf.ones([10]))\n",
    "beta2 = tf.Variable(tf.zeros([10]))\n",
    "BN2 = tf.nn.batch_normalization(z2_BN, bmean2, bvar2, beta2, scale2, epsilon)\n",
    "y_BN = BN2\n",
    "\n",
    "cross_entropy_BN = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_BN))\n",
    "train_step_BN = tf.train.GradientDescentOptimizer(0.3).minimize(cross_entropy_BN)\n",
    "\n",
    "correct_prediction_BN = tf.equal(tf.argmax(y_BN, 1), tf.argmax(y_, 1))\n",
    "accuracy_BN = tf.reduce_mean(tf.cast(correct_prediction_BN, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other objective Functions\n",
    "\n",
    "+ Mean Squared Error\n",
    "+ L2 normalization (which needs an regularization coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L2_lambda = 0.05\n",
    "\n",
    "mean_sq_err = tf.reduce_mean(tf.square(y - y_))\n",
    "L2_norm = cross_entropy + \\\n",
    "          L2_lambda * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other optimizers\n",
    "+ __tf.train.MomentumOptimizer__ (requires *momentum coefficient*)\n",
    "+ __tf.train.AdamOptimizer__\n",
    "+ Check the tensorflow website for further details, since many arguments can be given and are set to default when not specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.3\n",
    "momentum = 0.3\n",
    "\n",
    "train_step_momentum = tf.train.MomentumOptimizer(learning_rate, momentum).minimize(cross_entropy)\n",
    "train_step_adam = tf.train.AdamOptimizer().minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize variables, Training \n",
    "+ Inside the second for-loop, fetch 100 images and labels, and train the network\n",
    "+ A __training epoch__ is completed when the whole training data is trained once\n",
    "+ Continue training until *__validation accuracy starts to decrease__*, which signals __overfitting__\n",
    "+ __Look carefully at the values assigned for keep_prob! (Dropout)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-03ae80076c6c>:6: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "epoch 1: tr_NN 0.236, val_NN 0.309, tr_DO 0.714, val_DO 0.878, tr_BN 0.545, val_BN 0.638\n",
      "epoch 2: tr_NN 0.422, val_NN 0.499, tr_DO 0.899, val_DO 0.910, tr_BN 0.664, val_BN 0.660\n",
      "epoch 3: tr_NN 0.535, val_NN 0.580, tr_DO 0.915, val_DO 0.922, tr_BN 0.676, val_BN 0.673\n",
      "epoch 4: tr_NN 0.587, val_NN 0.605, tr_DO 0.924, val_DO 0.927, tr_BN 0.680, val_BN 0.676\n",
      "epoch 5: tr_NN 0.604, val_NN 0.610, tr_DO 0.929, val_DO 0.934, tr_BN 0.682, val_BN 0.684\n",
      "epoch 6: tr_NN 0.610, val_NN 0.617, tr_DO 0.933, val_DO 0.939, tr_BN 0.683, val_BN 0.687\n",
      "epoch 7: tr_NN 0.617, val_NN 0.619, tr_DO 0.937, val_DO 0.943, tr_BN 0.684, val_BN 0.693\n",
      "epoch 8: tr_NN 0.622, val_NN 0.626, tr_DO 0.940, val_DO 0.944, tr_BN 0.682, val_BN 0.695\n",
      "epoch 9: tr_NN 0.625, val_NN 0.626, tr_DO 0.942, val_DO 0.945, tr_BN 0.681, val_BN 0.688\n",
      "epoch 10: tr_NN 0.630, val_NN 0.633, tr_DO 0.944, val_DO 0.948, tr_BN 0.679, val_BN 0.692\n",
      "epoch 11: tr_NN 0.632, val_NN 0.630, tr_DO 0.946, val_DO 0.947, tr_BN 0.678, val_BN 0.687\n",
      "epoch 12: tr_NN 0.638, val_NN 0.638, tr_DO 0.948, val_DO 0.949, tr_BN 0.678, val_BN 0.682\n",
      "epoch 13: tr_NN 0.641, val_NN 0.640, tr_DO 0.950, val_DO 0.950, tr_BN 0.681, val_BN 0.681\n",
      "epoch 14: tr_NN 0.648, val_NN 0.650, tr_DO 0.951, val_DO 0.951, tr_BN 0.678, val_BN 0.680\n",
      "epoch 15: tr_NN 0.652, val_NN 0.654, tr_DO 0.952, val_DO 0.952, tr_BN 0.683, val_BN 0.683\n",
      "epoch 16: tr_NN 0.657, val_NN 0.663, tr_DO 0.953, val_DO 0.954, tr_BN 0.680, val_BN 0.681\n",
      "epoch 17: tr_NN 0.661, val_NN 0.669, tr_DO 0.954, val_DO 0.955, tr_BN 0.683, val_BN 0.684\n",
      "epoch 18: tr_NN 0.662, val_NN 0.666, tr_DO 0.955, val_DO 0.955, tr_BN 0.682, val_BN 0.683\n",
      "epoch 19: tr_NN 0.666, val_NN 0.659, tr_DO 0.956, val_DO 0.954, tr_BN 0.686, val_BN 0.687\n",
      "epoch 20: tr_NN 0.664, val_NN 0.667, tr_DO 0.957, val_DO 0.958, tr_BN 0.685, val_BN 0.694\n",
      "epoch 21: tr_NN 0.666, val_NN 0.669, tr_DO 0.957, val_DO 0.957, tr_BN 0.684, val_BN 0.690\n",
      "epoch 22: tr_NN 0.668, val_NN 0.676, tr_DO 0.958, val_DO 0.958, tr_BN 0.683, val_BN 0.689\n",
      "epoch 23: tr_NN 0.669, val_NN 0.651, tr_DO 0.959, val_DO 0.958, tr_BN 0.685, val_BN 0.691\n",
      "epoch 24: tr_NN 0.671, val_NN 0.658, tr_DO 0.959, val_DO 0.959, tr_BN 0.684, val_BN 0.695\n",
      "epoch 25: tr_NN 0.672, val_NN 0.663, tr_DO 0.960, val_DO 0.959, tr_BN 0.684, val_BN 0.691\n",
      "Seems like it starts to overfit, aborting the training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "list_vacc = list()\n",
    "list_vacc_BN = list()\n",
    "list_vacc_DO = list()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for i in range(num_epochs): # training epoch setting\n",
    "    list_tacc = list()\n",
    "    list_tacc_BN = list()\n",
    "    list_tacc_DO = list()\n",
    "    \n",
    "    for _ in range(550): # fetch & train 100 images at a time, which requires 550 iterations to complete an epoch\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100) # minibatch size setting\n",
    "        sess.run(train_step, feed_dict = {x: batch_xs, y_: batch_ys})\n",
    "        sess.run(train_step_DO, feed_dict = {x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "        sess.run(train_step_BN, feed_dict = {x: batch_xs, y_: batch_ys})\n",
    "        \n",
    "        list_tacc.append(accuracy.eval(feed_dict={x: batch_xs, y_: batch_ys}))\n",
    "        list_tacc_DO.append(accuracy_DO.eval(feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 1}))\n",
    "        list_tacc_BN.append(accuracy_BN.eval(feed_dict={x: batch_xs, y_: batch_ys}))\n",
    "        \n",
    "    vacc = accuracy.eval(feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})\n",
    "    vacc_DO = accuracy_DO.eval(feed_dict={x: mnist.validation.images, y_: mnist.validation.labels, keep_prob: 1})\n",
    "    vacc_BN = accuracy_BN.eval(feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})\n",
    "    list_vacc.append(vacc)\n",
    "    list_vacc_DO.append(vacc_DO)\n",
    "    list_vacc_BN.append(vacc_BN)\n",
    "    \n",
    "    print(\"epoch {}: tr_NN {:.3f}, val_NN {:.3f}, tr_DO {:.3f}, val_DO {:.3f}, tr_BN {:.3f}, val_BN {:.3f}\".format((i + 1), \\\n",
    "                                 np.mean(list_tacc), vacc, np.mean(list_tacc_DO), vacc_DO, np.mean(list_tacc_BN), vacc_BN))\n",
    "    , \n",
    "    if i > 10 and np.mean(list_vacc[-10:-5]) > np.mean(list_vacc[-5:]):\n",
    "        print(\"Seems like it starts to overfit, aborting the training\")\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "+ Check the test accuracy of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6515\n",
      "0.9535\n",
      "0.6932\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "print(sess.run(accuracy_DO, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1}))\n",
    "print(sess.run(accuracy_BN, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Session\n",
    "\n",
    "+ You must close the opened session.\n",
    "+ Or you can use \n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            tf.initialize_all_variables().run()\n",
    "            ... your code here using sess...\n",
    "            \n",
    "        # You don't need to close sess. It's already closed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
